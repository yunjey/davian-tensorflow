{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "In this tutorial, we are going to implement a neural machine translator for translating English to Korean. Our model is motivated by [this paper](https://arxiv.org/abs/1409.0473).\n",
    "\n",
    "\n",
    "![alt text](jpg/nmt2.jpg \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from utils import preprocess, decode_sequence\n",
    "from ops import lstm_cell, rnn_encoder, attention_mechanism\n",
    "from ops import rnn_decoder, rnn_decoder_test\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog ordinarily remains loyal to a considerate master\n",
      "개는 보통 사려 깊은 주인에게 충실합니다\n",
      "\n",
      "Class distinctions between people have no part in a dog's life\n",
      "사람들 사이의 계급 구분은 개의 삶에 아무런 영향을 미치지 않습니다\n",
      "\n",
      "It can be a faithful companion to either rich or poor\n",
      "부자 나 가난한 자에게 충실한 동반자가 될 수 있습니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_sequences = open('data/source.txt').readlines()\n",
    "target_sequences = open('data/target.txt').readlines()\n",
    "\n",
    "k = 3\n",
    "for eng, kor in zip(source_sequences[0:k], target_sequences[0:k]):\n",
    "    print eng, kor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source, source_mask, word_to_idx_src, source_seq_length, source_vocab_size = preprocess(source_sequences)\n",
    "target, target_mask, word_to_idx_trg, target_seq_length, target_vocab_size = preprocess(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog ordinarily remains loyal to a considerate master .\n",
      "개는 보통 사려 깊은 주인에게 충실합니다 .\n"
     ]
    }
   ],
   "source": [
    "eng = decode_sequence(source[0:1], word_to_idx_src)\n",
    "kor =  decode_sequence(target[0:1], word_to_idx_trg)\n",
    "\n",
    "for e, k in zip(eng, kor):\n",
    "    print e\n",
    "    print k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dim_h = 128\n",
    "dim_emb = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "# weights and biases for rnn encoder \n",
    "params['w_encoder'] = tf.Variable(tf.random_normal(shape=[dim_h + dim_emb, dim_h*4], stddev=0.1), name='w_encoder')\n",
    "params['b_encoder'] = tf.Variable(tf.zeros(shape=[dim_h*4]), name='b_encoder')\n",
    "\n",
    "# weights and biases for rnn decoder\n",
    "params['w_decoder'] = tf.Variable(tf.random_normal(shape=[dim_h*2 + dim_emb, dim_h*4], stddev=0.1), name='w_decoder')\n",
    "params['b_decoder'] = tf.Variable(tf.zeros(shape=[dim_h*4]), name='w_decoder')\n",
    "\n",
    "# weigths and biases for attention mechanism\n",
    "params['w1_att'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w1_att')\n",
    "params['w2_att'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w2_att')\n",
    "params['b_att'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b_att')\n",
    "params['w3_att'] = tf.Variable(tf.random_normal(shape=[dim_h, 1], stddev=0.1), name='w3_att')\n",
    "\n",
    "# embedding matrices for source and target languages\n",
    "params['w_emb_src'] = tf.Variable(tf.random_uniform(shape=[source_vocab_size, dim_emb], minval=-1.0, maxval=1.0),\n",
    "                                 name='w_emb_src')\n",
    "params['w_emb_trg'] = tf.Variable(tf.random_uniform(shape=[target_vocab_size, dim_emb], minval=-1.0, maxval=1.0),\n",
    "                                 name='w_emb_trg')\n",
    "\n",
    "\n",
    "# weigths and biases for initializing initial cell and hidden state in decoder\n",
    "params['w_init_c'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w_init_c')\n",
    "params['b_init_c'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b_init_c')\n",
    "params['w_init_h'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w_init_h')\n",
    "params['b_init_h'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b_init_h')\n",
    "\n",
    "# weights and biases for computing logits (include softmax layer)\n",
    "params['w1_logit'] = tf.Variable(tf.random_normal(shape=[dim_h, dim_h], stddev=0.1), name='w1_logit')\n",
    "params['b1_logit'] = tf.Variable(tf.zeros(shape=[dim_h]), name='b1_logit')\n",
    "params['w2_logit'] = tf.Variable(tf.random_normal(shape=[dim_h, target_vocab_size], stddev=0.1), name='w2_logit')\n",
    "params['b2_logit'] = tf.Variable(tf.zeros(shape=[target_vocab_size]), name='b2_logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_source = tf.placeholder(dtype=tf.int64, shape=[None, source_seq_length], name='source_seq')\n",
    "tf_source_mask = tf.placeholder(dtype=tf.int64, shape=[None, source_seq_length], name='source_mask')\n",
    "tf_target = tf.placeholder(dtype=tf.int64, shape=[None, target_seq_length], name='target_seq')\n",
    "tf_target_mask = tf.placeholder(dtype=tf.int64, shape=[None, target_seq_length], name='target_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "h_encoded = rnn_encoder(tf_source, params) # (batch_size, source_seq_length, dim_h)\n",
    "\n",
    "# decoder (train mode)\n",
    "loss = rnn_decoder(tf_target, h_encoded, tf_source_mask, tf_target_mask, params)\n",
    "\n",
    "# decoder (test mode)\n",
    "sampled_seq = rnn_decoder_test(h_encoded, tf_source_mask, word_to_idx_trg, params) # (batch_size, target_seq_length-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train op\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads_and_vars = list(zip(grads, tf.trainable_variables()))\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "\n",
    "# add summary op   \n",
    "tf.scalar_summary('batch_loss', loss)\n",
    "for var in tf.trainable_variables():\n",
    "    tf.histogram_summary(var.op.name, var)\n",
    "for grad, var in grads_and_vars:\n",
    "    tf.histogram_summary(var.op.name+'/gradient', grad)\n",
    "\n",
    "summary_op = tf.merge_all_summaries() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss at epoch 10: 95.098\n",
      "원문: here i was in contact with what i most wanted, life in the raw .\n",
      "번역: 그것은 그것은 그것은 <PAD> .\n",
      "\n",
      "loss at epoch 20: 52.845\n",
      "원문: dogs have endeared themselves to many over the years .\n",
      "번역: 그것은 놀라운 이야기였습니다, 이야기였습니다, 그것은 관심을 .\n",
      "\n",
      "loss at epoch 30: 23.770\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 .\n",
      "\n",
      "loss at epoch 40: 10.007\n",
      "원문: dogs have endeared themselves to many over the years .\n",
      "번역: 개들은 수년에 걸쳐 많은 사람들에게 사랑 받아 왔습니다 .\n",
      "\n",
      "loss at epoch 50: 2.791\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 자극했습니다 .\n",
      "\n",
      "loss at epoch 60: 0.788\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 자극했습니다 .\n",
      "\n",
      "loss at epoch 70: 0.187\n",
      "원문: in those three years i must have witnessed pretty well every emotion of which man is capable .\n",
      "번역: 그 3 년 동안 나는 인간이 할 수있는 모든 감정을 꽤 잘 목격했을 것입니다 .\n",
      "\n",
      "loss at epoch 80: 0.043\n",
      "원문: here i was in contact with what i most wanted, life in the raw .\n",
      "번역: 여기에서 나는 내가 가장 원했던 것과 연락을 취했다 .\n",
      "\n",
      "loss at epoch 90: 0.011\n",
      "원문: it excited the novelist in me .\n",
      "번역: 그것은 저의 소설가를 흥분 시켰습니다 .\n",
      "\n",
      "loss at epoch 100: 0.004\n",
      "원문: it was an amazing story, and it piqued my interest .\n",
      "번역: 그것은 놀라운 이야기였습니다, 그리고 그것은 내 관심을 자극했습니다 .\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "tot_batch = len(source)\n",
    "num_iter_per_epoch = int(np.ceil(tot_batch / batch_size)) \n",
    "log_path = 'log/'\n",
    "print_every = 10\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True \n",
    "with tf.Session(config=config) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    summary_writer = tf.train.SummaryWriter(log_path, graph=tf.get_default_graph())\n",
    "    for e in range(num_epoch):\n",
    "        # TODO: random shuffle train dataset\n",
    "        for i in range(num_iter_per_epoch):\n",
    "            # get batch data\n",
    "            source_batch = source[i*batch_size:(i+1)*batch_size, :]\n",
    "            source_mask_batch = source_mask[i*batch_size:(i+1)*batch_size, :]\n",
    "            target_batch = target[i*batch_size:(i+1)*batch_size, :]\n",
    "            target_mask_batch = target_mask[i*batch_size:(i+1)*batch_size, :]\n",
    "            \n",
    "            # train op\n",
    "            feed_dict={tf_source: source_batch, tf_source_mask: source_mask_batch, \n",
    "                       tf_target: target_batch, tf_target_mask: target_mask_batch}\n",
    "            _, l = sess.run(fetches=[train_op, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                summary = sess.run(summary_op, feed_dict)\n",
    "                summary_writer.add_summary(summary, e*num_iter_per_epoch + i)\n",
    "                \n",
    "        # print sampled sequences\n",
    "        if (e+1) % print_every == 0:\n",
    "            print (\"\\nloss at epoch %d: %.3f\" %(e+1, l))\n",
    "            \n",
    "            feed_dict={tf_source: source, tf_source_mask: source_mask}\n",
    "            np_sampled_seq = sess.run(fetches=sampled_seq, feed_dict=feed_dict) \n",
    "            \n",
    "            eng = decode_sequence(sequences=source, word_to_idx=word_to_idx_src)\n",
    "            kor = decode_sequence(sequences=np_sampled_seq, word_to_idx=word_to_idx_trg)\n",
    "            \n",
    "            rand_idx = np.random.randint(tot_batch)\n",
    "            print '원문: %s' %eng[rand_idx]\n",
    "            print '번역: %s' %kor[rand_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TODO\n",
    "1) Implement bidirectional recurrent neural network in rnn_encoder function. (30 points)\n",
    "\n",
    "2) Build large dataset to translate English to Korean. (30 points)\n",
    "\n",
    "3) Visualize attention weights to clarify alignment between source and target sequences. (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
